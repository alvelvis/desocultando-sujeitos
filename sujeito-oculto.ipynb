{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./udpipe-1.2.0 --tokenize --tag --parse bosque-ud-2.6.udpipe dhbb.raw > dhbb.conllu\n",
    "! ./udpipe-1.2.0 --tokenize --tag --parse bosque-ud-2.6.udpipe obras.raw > obras.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resource\n",
    "gb = 2**30\n",
    "resource.setrlimit(resource.RLIMIT_AS, (10*gb,10*gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo variáveis iniciais\n",
    "import estrutura_ud\n",
    "import os\n",
    "\n",
    "root_deprel = [\"root\"]\n",
    "subordinate_deprel = [\"acl:relcl\", \"ccomp\", \"advcl\"]\n",
    "max_examples = 20\n",
    "\n",
    "path = {\n",
    "    'bosque': \"bosque-ud-2.6.conllu\",\n",
    "    'dhbb': \"dhbb.conllu\",\n",
    "    \"obras\": \"obras.conllu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bosque ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/elvis/desocultando-sujeitos/estrutura_ud.py\", line 108, in build\n",
      "  File \"/home/elvis/desocultando-sujeitos/estrutura_ud.py\", line 58, in build\n",
      "MemoryError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n",
      "  File \"<ipython-input-3-8328003cc172>\", line 7, in <module>\n",
      "  File \"/home/elvis/desocultando-sujeitos/estrutura_ud.py\", line 219, in load\n",
      "  File \"/home/elvis/desocultando-sujeitos/estrutura_ud.py\", line 174, in build\n",
      "  File \"/home/elvis/desocultando-sujeitos/estrutura_ud.py\", line 116, in build\n",
      "TypeError: unsupported operand type(s) for +: 'MemoryError' and 'str'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3360, in run_code\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2022, in showtraceback\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1985, in _get_exc_info\n",
      "MemoryError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2895, in _run_cell\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3072, in run_cell_async\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3282, in run_ast_nodes\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2022, in showtraceback\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1985, in _get_exc_info\n",
      "MemoryError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/tornado/gen.py\", line 762, in run\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/tornado/gen.py\", line 769, in run\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/tornado/gen.py\", line 762, in run\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2867, in run_cell\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2900, in _run_cell\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2022, in showtraceback\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1985, in _get_exc_info\n",
      "MemoryError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/logging/__init__.py\", line 994, in emit\n",
      "  File \"/usr/lib/python3.6/logging/__init__.py\", line 840, in format\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/traitlets/config/application.py\", line 117, in format\n",
      "  File \"/usr/lib/python3.6/logging/__init__.py\", line 585, in format\n",
      "  File \"/usr/lib/python3.6/logging/__init__.py\", line 539, in formatException\n",
      "MemoryError\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 612, in start\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n",
      "  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n",
      "  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/tornado/ioloop.py\", line 688, in <lambda>\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/tornado/gen.py\", line 814, in inner\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/tornado/gen.py\", line 775, in run\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/tornado/gen.py\", line 250, in wrapper\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/tornado/gen.py\", line 741, in __init__\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/tornado/gen.py\", line 769, in run\n",
      "  File \"/home/elvis/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 270, in dispatch_shell\n",
      "Message: 'Exception in message handler:'\n",
      "Arguments: ()\n"
     ]
    }
   ],
   "source": [
    "# Carregamento dos corpora (demora muito!)\n",
    "bosque = estrutura_ud.Corpus(recursivo=True)\n",
    "dhbb = estrutura_ud.Corpus(recursivo=True)\n",
    "obras = estrutura_ud.Corpus(recursivo=True)\n",
    "bosque.load(path[\"bosque\"])\n",
    "print(\"bosque ok\")\n",
    "dhbb.load(path[\"dhbb\"])\n",
    "print(\"dhbb ok\")\n",
    "obras.load(path[\"obras\"])\n",
    "print(\"obras ok\")\n",
    "\n",
    "corpora = {\n",
    "    'dhbb': dhbb,\n",
    "    'obras': obras,\n",
    "    'bosque': bosque\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Estatísticas dos corpora\n",
    "stats_corpora = {}\n",
    "for corpus in corpora:\n",
    "    stats_corpora[corpus] = {\n",
    "        \"size\": os.path.getsize(path[corpus])/1024, \n",
    "        \"n_tokens\": 0, \n",
    "        \"n_sentences\": 0,\n",
    "    }\n",
    "    \n",
    "for corpus in corpora:\n",
    "    for sentence in corpora[corpus].sentences.values():\n",
    "        stats_corpora[corpus]['n_sentences'] += 1\n",
    "        for token in sentence.tokens:\n",
    "            if not '-' in token.id:\n",
    "                stats_corpora[corpus]['n_tokens'] += 1\n",
    "    \n",
    "print(stats_corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Números de orações\n",
    "total_deprel = {}\n",
    "for corpus in corpora:\n",
    "    total_deprel[corpus] = {\n",
    "        \"|\".join(root_deprel): 0,\n",
    "        \"|\".join(subordinate_deprel): 0\n",
    "    }\n",
    "    for sentence in corpora[corpus].sentences.values():\n",
    "        for token in sentence.tokens:\n",
    "            if token.deprel in root_deprel:\n",
    "                total_deprel[corpus][\"|\".join(root_deprel)] += 1\n",
    "            if token.deprel in subordinate_deprel:\n",
    "                total_deprel[corpus][\"|\".join(subordinate_deprel)] += 1\n",
    "print(total_deprel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Busca ingênua tanto em root quanto em subordinada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clause_without_subject(sentences, deprel_list):\n",
    "    clauses_without_subject = {}\n",
    "    for sent_id, sentence in sentences.items():\n",
    "        for t, token in enumerate(sentence.tokens):\n",
    "            if token.deprel in deprel_list:\n",
    "                has_subj = False\n",
    "                for _token in sentence.tokens:\n",
    "                    if _token.dephead == token.id and _token.deprel in [\"nsubj\", \"csubj\", \"nsubj:pass\"]:\n",
    "                        has_subj = True\n",
    "                        break\n",
    "                if not has_subj:\n",
    "                    if not sent_id in clauses_without_subject:\n",
    "                        clauses_without_subject[sent_id] = []\n",
    "                    clauses_without_subject[sent_id].append(t)\n",
    "    return clauses_without_subject\n",
    "\n",
    "root_without_subject = {}\n",
    "subordinate_without_subject = {}\n",
    "for corpus in corpora:\n",
    "    root_without_subject[corpus] = find_clause_without_subject(corpora[corpus].sentences, root_deprel)\n",
    "    subordinate_without_subject[corpus] = find_clause_without_subject(corpora[corpus].sentences, subordinate_deprel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for corpus in corpora:\n",
    "    print(\"\\n{} - orações principais com sujeito oculto ingênuo: {} / {}\".format(corpus, sum([len(x) for x in root_without_subject[corpus].values()]), total_deprel[corpus][\"|\".join(root_deprel)]))\n",
    "    print(\"{} - orações subordinadas com sujeito oculto ingênuo: {} / {}\".format(corpus, sum([len(x) for x in subordinate_without_subject[corpus].values()]), total_deprel[corpus][\"|\".join(subordinate_deprel)]))\n",
    "\n",
    "for corpus in corpora:\n",
    "    i = 0\n",
    "    ii = 0\n",
    "    print(\"\\n=== {} exemplos de sujeito oculto na oração principal em {} ===\".format(max_examples, corpus))\n",
    "    for sent_id, tokens in root_without_subject[corpus].items():\n",
    "        i += len(tokens)\n",
    "        print(sent_id + \": \" + \" \".join([x.word if t not in tokens else \"*\" + x.word + \"*\" for t, x in enumerate(corpora[corpus].sentences[sent_id].tokens) if not '-' in x.id]))\n",
    "        if i >= max_examples:\n",
    "            break\n",
    "    print(\"\\n=== {} exemplos de sujeito oculto na oração subordinada em {} ===\".format(max_examples, corpus))\n",
    "    for sent_id, tokens in subordinate_without_subject[corpus].items():\n",
    "        ii += len(tokens)\n",
    "        print(sent_id + \": \" + \" \".join([x.word if t not in tokens else \"*\" + x.word + \"*\" for t, x in enumerate(corpora[corpus].sentences[sent_id].tokens) if not '-' in x.id]))\n",
    "        if ii >= max_examples:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtro A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_haver_impessoal(sentences, list_tokens):\n",
    "    haver_impessoal = {}\n",
    "    for sent_id in list_tokens:\n",
    "        sentence = sentences[sent_id]\n",
    "        for t, token in enumerate(sentence.tokens):\n",
    "            if t in list_tokens[sent_id] and token.lemma == \"haver\" and \"Number=Sing\" in token.feats and \"Person=3\" in token.feats:\n",
    "                if not sent_id in haver_impessoal:\n",
    "                    haver_impessoal[sent_id] = []\n",
    "                haver_impessoal[sent_id].append(t)\n",
    "    return haver_impessoal\n",
    "                \n",
    "root_haver_impessoal = {}\n",
    "subordinate_haver_impessoal = {}\n",
    "for corpus in corpora:\n",
    "    root_haver_impessoal[corpus] = find_haver_impessoal(corpora[corpus].sentences, root_without_subject[corpus])\n",
    "    subordinate_haver_impessoal[corpus] = find_haver_impessoal(corpora[corpus].sentences, subordinate_without_subject[corpus])\n",
    "    \n",
    "    for sentence in root_haver_impessoal[corpus]:\n",
    "        for token in root_haver_impessoal[corpus][sentence]:\n",
    "            root_without_subject[corpus][sentence].remove(token)\n",
    "    for sentence in subordinate_haver_impessoal[corpus]:\n",
    "        for token in subordinate_haver_impessoal[corpus][sentence]:\n",
    "            subordinate_without_subject[corpus][sentence].remove(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for corpus in corpora:\n",
    "    print(\"\\n{} - orações principais com sujeito oculto e haver impessoal: {}\".format(corpus, sum([len(x) for x in root_haver_impessoal[corpus].values()])))\n",
    "    print(\"{} - orações subordinadas com sujeito oculto e haver impessoal: {}\".format(corpus, sum([len(x) for x in subordinate_haver_impessoal[corpus].values()])))\n",
    "\n",
    "for corpus in corpora:\n",
    "    i = 0\n",
    "    ii = 0\n",
    "    print(\"\\n=== {} exemplos de haver impessoal na oração principal em {} ===\".format(max_examples, corpus))\n",
    "    for sent_id, tokens in root_haver_impessoal[corpus].items():\n",
    "        i += len(tokens)\n",
    "        print(sent_id + \": \" + \" \".join([x.word if t not in tokens else \"*\" + x.word + \"*\" for t, x in enumerate(corpora[corpus].sentences[sent_id].tokens) if not '-' in x.id]))\n",
    "        if i >= max_examples:\n",
    "            break\n",
    "    print(\"\\n=== {} exemplos de haver impessoal na oração subordinada em {} ===\".format(max_examples, corpus))\n",
    "    for sent_id, tokens in subordinate_haver_impessoal[corpus].items():\n",
    "        ii += len(tokens)\n",
    "        print(sent_id + \": \" + \" \".join([x.word if t not in tokens else \"*\" + x.word + \"*\" for t, x in enumerate(corpora[corpus].sentences[sent_id].tokens) if not '-' in x.id]))\n",
    "        if ii >= max_examples:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtro B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nominals(sentences, list_tokens):\n",
    "    nominals = {}\n",
    "    for sent_id in list_tokens:\n",
    "        sentence = sentences[sent_id]\n",
    "        for t, token in enumerate(sentence.tokens):\n",
    "            if t in list_tokens[sent_id] and token.upos != \"VERB\":\n",
    "                is_cop = False\n",
    "                for _token in sentence.tokens:\n",
    "                    if _token.head_token.id == token.id and _token.deprel == \"cop\":\n",
    "                        is_cop = True\n",
    "                        break\n",
    "                if not is_cop:\n",
    "                    if not sent_id in nominals:\n",
    "                        nominals[sent_id] = []\n",
    "                    nominals[sent_id].append(t)\n",
    "    return nominals\n",
    "                \n",
    "root_nominals = {}\n",
    "for corpus in corpora:\n",
    "    root_nominals[corpus] = find_nominals(corpora[corpus].sentences, root_without_subject[corpus])\n",
    "    \n",
    "    for sentence in root_nominals[corpus]:\n",
    "        for token in root_nominals[corpus][sentence]:\n",
    "            root_without_subject[corpus][sentence].remove(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for corpus in corpora:\n",
    "    print(\"\\n{} - orações principais com sujeito oculto e nominal: {}\".format(corpus, sum([len(x) for x in root_nominals[corpus].values()])))\n",
    "\n",
    "for corpus in corpora:\n",
    "    i = 0\n",
    "    print(\"\\n=== {} exemplos de nominais como oração principal em {} ===\".format(max_examples, corpus))\n",
    "    for sent_id, tokens in root_nominals[corpus].items():\n",
    "        i += len(tokens)\n",
    "        print(sent_id + \": \" + \" \".join([x.word if t not in tokens else \"*\" + x.word + \"*\" for t, x in enumerate(corpora[corpus].sentences[sent_id].tokens) if not '-' in x.id]))\n",
    "        if i >= max_examples:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtro C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fenomenos_natureza = \"chover|ventar|nevar|chuviscar|garoar|gear|anoitecer|amanhecer|entardecer|relampejar|trovejar\".split(\"|\")\n",
    "def find_natureza(sentences, list_tokens):\n",
    "    natureza = {}\n",
    "    for sent_id in list_tokens:\n",
    "        sentence = sentences[sent_id]\n",
    "        for t, token in enumerate(sentence.tokens):\n",
    "            if t in list_tokens[sent_id] and token.lemma in fenomenos_natureza:\n",
    "                if not sent_id in natureza:\n",
    "                    natureza[sent_id] = []\n",
    "                natureza[sent_id].append(t)\n",
    "    return natureza\n",
    "                \n",
    "root_natureza = {}\n",
    "subordinate_natureza = {}\n",
    "for corpus in corpora:\n",
    "    root_natureza[corpus] = find_natureza(corpora[corpus].sentences, root_without_subject[corpus])\n",
    "    subordinate_natureza[corpus] = find_natureza(corpora[corpus].sentences, subordinate_without_subject[corpus])\n",
    "    \n",
    "    for sentence in root_natureza[corpus]:\n",
    "        for token in root_natureza[corpus][sentence]:\n",
    "            root_without_subject[corpus][sentence].remove(token)\n",
    "    for sentence in subordinate_natureza[corpus]:\n",
    "        for token in subordinate_natureza[corpus][sentence]:\n",
    "            subordinate_without_subject[corpus][sentence].remove(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for corpus in corpora:\n",
    "    print(\"\\n{} - orações principais com sujeito oculto e fenômeno natureza: {}\".format(corpus, sum([len(x) for x in root_natureza[corpus].values()])))\n",
    "    print(\"{} - orações subordinadas com sujeito oculto e fenômeno natureza: {}\".format(corpus, sum([len(x) for x in subordinate_natureza[corpus].values()])))\n",
    "    \n",
    "for corpus in corpora:\n",
    "    i = 0\n",
    "    ii = 0\n",
    "    print(\"\\n=== {} exemplos de fenômeno natureza como oração principal em {} ===\".format(max_examples, corpus))\n",
    "    for sent_id, tokens in root_natureza[corpus].items():\n",
    "        i += len(tokens)\n",
    "        print(sent_id + \": \" + \" \".join([x.word if t not in tokens else \"*\" + x.word + \"*\" for t, x in enumerate(corpora[corpus].sentences[sent_id].tokens) if not '-' in x.id]))\n",
    "        if i >= max_examples:\n",
    "            break\n",
    "    print(\"\\n=== {} exemplos de fenômeno natureza como oração subordinada em {} ===\".format(max_examples, corpus))\n",
    "    for sent_id, tokens in subordinate_natureza[corpus].items():\n",
    "        ii += len(tokens)\n",
    "        print(sent_id + \": \" + \" \".join([x.word if t not in tokens else \"*\" + x.word + \"*\" for t, x in enumerate(corpora[corpus].sentences[sent_id].tokens) if not '-' in x.id]))\n",
    "        if ii >= max_examples:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtro D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_se(sentences, list_tokens):\n",
    "    se = {\"1\": {}, \"2\": {}, \"3\": {}}\n",
    "    for sent_id in list_tokens:\n",
    "        sentence = sentences[sent_id]\n",
    "        for t, token in enumerate(sentence.tokens):\n",
    "            if token.head_token.id != \"_\" and token.lemma == \"se\" and token.upos == \"PRON\" and \"Gender=Unsp\" in token.feats and sentence.map_token_id[token.head_token.id] in list_tokens[sent_id]:\n",
    "                if not sent_id in se[\"1\"]:\n",
    "                    se[\"1\"][sent_id] = []\n",
    "                se[\"1\"][sent_id].append(sentence.map_token_id[token.head_token.id])\n",
    "            if token.head_token.id != \"_\" and token.lemma == \"se\" and token.upos == \"PRON\" and \"VerbForm=Inf\" in token.head_token.feats and sentence.map_token_id[token.head_token.id] in list_tokens[sent_id]:\n",
    "                if not sent_id in se[\"2\"]:\n",
    "                    se[\"2\"][sent_id] = []\n",
    "                se[\"2\"][sent_id].append(sentence.map_token_id[token.head_token.id])\n",
    "            if token.head_token.id != \"_\" and token.lemma == \"se\" and token.upos == \"PRON\" and sentence.map_token_id[token.head_token.id] in list_tokens[sent_id]:\n",
    "                for _token in sentence.tokens:\n",
    "                    if _token.deprel == \"case\" and _token.head_token.head_token.id == token.head_token.id:\n",
    "                        if not sent_id in se[\"3\"]:\n",
    "                            se[\"3\"][sent_id] = []\n",
    "                        se[\"3\"][sent_id].append(sentence.map_token_id[token.head_token.id])\n",
    "                        break\n",
    "    return se\n",
    "                \n",
    "root_se = {}\n",
    "subordinate_se = {}\n",
    "for corpus in corpora:\n",
    "    if corpus in [\"bosque\"]:\n",
    "        root_se[corpus] = find_se(corpora[corpus].sentences, root_without_subject[corpus])\n",
    "        subordinate_se[corpus] = find_se(corpora[corpus].sentences, subordinate_without_subject[corpus])\n",
    "        \n",
    "        for filtro in [\"1\", \"2\", \"3\"]:\n",
    "            for sentence in root_se[corpus][filtro]:\n",
    "                for token in root_se[corpus][filtro][sentence]:\n",
    "                    if token in root_without_subject[corpus][sentence]:\n",
    "                        root_without_subject[corpus][sentence].remove(token)\n",
    "            if filtro in [\"1\"]:\n",
    "                for sentence in subordinate_se[corpus][filtro]:\n",
    "                    for token in subordinate_se[corpus][filtro][sentence]:\n",
    "                        if token in subordinate_without_subject[corpus][sentence]:\n",
    "                            subordinate_without_subject[corpus][sentence].remove(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for corpus in corpora:\n",
    "    if corpus in [\"bosque\"]:\n",
    "        for filtro in [\"1\", \"2\", \"3\"]:\n",
    "            print(\"\\n{} - orações principais com sujeito oculto e se (filtro {}): {}\".format(corpus, filtro, sum([len(x) for x in root_se[corpus][filtro].values()])))\n",
    "            if filtro in [\"1\"]:\n",
    "                print(\"{} - orações subordinadas com sujeito oculto e se (filtro {}): {}\".format(corpus, filtro, sum([len(x) for x in subordinate_se[corpus][filtro].values()])))\n",
    "\n",
    "for corpus in corpora:\n",
    "    if corpus in [\"bosque\"]:\n",
    "        for filtro in [\"1\", \"2\", \"3\"]:\n",
    "            i = 0\n",
    "            ii = 0\n",
    "            print(\"\\n=== {} exemplos de se (filtro {}) como filho de oração principal em {} ===\".format(max_examples, filtro, corpus))\n",
    "            for sent_id, tokens in root_se[corpus][filtro].items():\n",
    "                i += len(tokens)\n",
    "                print(sent_id + \": \" + \" \".join([x.word if t not in tokens else \"*\" + x.word + \"*\" for t, x in enumerate(corpora[corpus].sentences[sent_id].tokens) if not '-' in x.id]))\n",
    "                if i >= max_examples:\n",
    "                    break\n",
    "            if filtro in [\"1\"]:\n",
    "                print(\"\\n=== {} exemplos de se (filtro {}) como filho de oração subordinada em {} ===\".format(max_examples, filtro, corpus))\n",
    "                for sent_id, tokens in subordinate_se[corpus][filtro].items():\n",
    "                    ii += len(tokens)\n",
    "                    print(sent_id + \": \" + \" \".join([x.word if t not in tokens else \"*\" + x.word + \"*\" for t, x in enumerate(corpora[corpus].sentences[sent_id].tokens) if not '-' in x.id]))\n",
    "                    if ii >= max_examples:\n",
    "                        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtro E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tratarse(sentences, list_tokens):\n",
    "    tratarse = {}\n",
    "    for sent_id in list_tokens:\n",
    "        sentence = sentences[sent_id]\n",
    "        for t, token in enumerate(sentence.tokens):\n",
    "            if token.lemma == \"tratar\" and token.next_token.lemma == \"se\":\n",
    "                if not sent_id in tratarse:\n",
    "                    tratarse[sent_id].append(sentence.map_token_id[token.id])\n",
    "    return se\n",
    "                \n",
    "root_tratarse = {}\n",
    "subordinate_tratarse = {}\n",
    "for corpus in corpora:\n",
    "    if corpus in [\"dhbb\", \"obras\"]:\n",
    "        root_tratarse[corpus] = find_tratarse(corpora[corpus].sentences, root_without_subject[corpus])\n",
    "        subordinate_tratarse[corpus] = find_tratarse(corpora[corpus].sentences, subordinate_without_subject[corpus])\n",
    "        \n",
    "        for sentence in root_tratarse[corpus][filtro]:\n",
    "            for token in root_tratarse[corpus][filtro][sentence]:\n",
    "                if token in root_without_subject[corpus][sentence]:\n",
    "                    root_without_subject[corpus][sentence].remove(token)\n",
    "        for sentence in subordinate_tratarse[corpus][filtro]:\n",
    "            for token in subordinate_tratarse[corpus][filtro][sentence]:\n",
    "                if token in subordinate_without_subject[corpus][sentence]:\n",
    "                    subordinate_without_subject[corpus][sentence].remove(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for corpus in corpora:\n",
    "    if corpus in [\"dhbb\", \"obras\"]:\n",
    "        print(\"\\n{} - orações principais com sujeito oculto e tratar-se: {}\".format(corpus, sum([len(x) for x in root_tratarse[corpus][filtro].values()])))\n",
    "        print(\"{} - orações subordinadas com sujeito oculto e tratar-se: {}\".format(corpus, sum([len(x) for x in subordinate_tratarse[corpus][filtro].values()])))\n",
    "\n",
    "for corpus in corpora:\n",
    "    i = 0\n",
    "    ii = 0\n",
    "    print(\"\\n=== {} exemplos de tratar-se como oração principal em {} ===\".format(max_examples, corpus))\n",
    "    for sent_id, tokens in root_tratarse[corpus][filtro].items():\n",
    "        i += len(tokens)\n",
    "        print(sent_id + \": \" + \" \".join([x.word if t not in tokens else \"*\" + x.word + \"*\" for t, x in enumerate(corpora[corpus].sentences[sent_id].tokens) if not '-' in x.id]))\n",
    "        if i >= max_examples:\n",
    "            break\n",
    "    print(\"\\n=== {} exemplos de tratar-se como oração subordinada em {} ===\".format(max_examples, corpus))\n",
    "    for sent_id, tokens in subordinate_tratarse[corpus][filtro].items():\n",
    "        ii += len(tokens)\n",
    "        print(sent_id + \": \" + \" \".join([x.word if t not in tokens else \"*\" + x.word + \"*\" for t, x in enumerate(corpora[corpus].sentences[sent_id].tokens) if not '-' in x.id]))\n",
    "        if ii >= max_examples:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contagem final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for corpus in corpora:\n",
    "    print(\"\\n{} - orações principais com sujeito oculto final: {} / {}\".format(corpus, sum([len(x) for x in root_without_subject[corpus].values()]), total_deprel[corpus][\"|\".join(root_deprel)]))\n",
    "    print(\"{} - orações subordinadas com sujeito oculto final: {} / {}\".format(corpus, sum([len(x) for x in subordinate_without_subject[corpus].values()]), total_deprel[corpus][\"|\".join(subordinate_deprel)]))\n",
    "\n",
    "for corpus in corpora:\n",
    "    i = 0\n",
    "    ii = 0\n",
    "    print(\"\\n=== {} exemplos de sujeito oculto na oração principal em {} ===\".format(max_examples, corpus))\n",
    "    for sent_id, tokens in root_without_subject[corpus].items():\n",
    "        if len(tokens):\n",
    "            i += len(tokens)\n",
    "            print(sent_id + \": \" + \" \".join([x.word if t not in tokens else \"*\" + x.word + \"*\" for t, x in enumerate(corpora[corpus].sentences[sent_id].tokens) if not '-' in x.id]))\n",
    "            if i >= max_examples:\n",
    "                break\n",
    "    print(\"\\n=== {} exemplos de sujeito oculto na oração subordinada em {} ===\".format(max_examples, corpus))\n",
    "    for sent_id, tokens in subordinate_without_subject[corpus].items():\n",
    "        if len(tokens):\n",
    "            ii += len(tokens)\n",
    "            print(sent_id + \": \" + \" \".join([x.word if t not in tokens else \"*\" + x.word + \"*\" for t, x in enumerate(corpora[corpus].sentences[sent_id].tokens) if not '-' in x.id]))\n",
    "            if ii >= max_examples:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle Bosque para posterior reconstituição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "for corpus in path:\n",
    "    for sentence in list(root_without_subject[corpus].keys()):\n",
    "        if not root_without_subject[corpus][sentence]:\n",
    "            del root_without_subject[corpus][sentence]\n",
    "        \n",
    "    for sentence in list(subordinate_without_subject[corpus].keys()):\n",
    "        if not subordinate_without_subject[corpus][sentence]:\n",
    "            del subordinate_without_subject[corpus][sentence]\n",
    "\n",
    "    if corpus == \"bosque\":\n",
    "        with open(\"{}_root.p\".format(corpus), \"wb\") as f:\n",
    "            pickle.dump(root_without_subject[corpus], f)\n",
    "        with open(\"{}_subordinate.p\".format(corpus), \"wb\") as f:\n",
    "            pickle.dump(subordinate_without_subject[corpus], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstituição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import estrutura_ud\n",
    "import pickle\n",
    "\n",
    "with open(\"bosque_root.p\", \"rb\") as f:\n",
    "    root = pickle.load(f)\n",
    "with open(\"bosque_subordinate.p\", \"rb\") as f:\n",
    "    subordinate = pickle.load(f)\n",
    "    \n",
    "len_root = sum([len(root[x]) for x in root])\n",
    "len_subordinate = sum([len(subordinate[x]) for x in subordinate])\n",
    "    \n",
    "bosque = estrutura_ud.Corpus(recursivo=True)\n",
    "bosque.load(\"bosque-ud-2.6.conllu\")\n",
    "\n",
    "dic_pronouns = {\n",
    "    '1Sing': ['eu', 'eu'],\n",
    "    '1SingMale': ['eu', 'eu'],\n",
    "    '1SingFem': ['eu', 'eu'],\n",
    "    '2Sing': ['tu', 'tu'],\n",
    "    '2SingMale': ['tu', 'tu'],\n",
    "    '2SingFem': ['tu', 'tu'],\n",
    "    '3SingMale': ['ele', 'ele'],\n",
    "    '3SingFem': ['ela', 'ela'],\n",
    "    '1Plur': ['nós', 'nós'],\n",
    "    '1PlurMale': ['nós', 'nós'],\n",
    "    '1PlurFem': ['nós', 'nós'],\n",
    "    '2Plur': ['vós', 'vós'],\n",
    "    '2PlurMale': ['vós', 'vós'],\n",
    "    '2PlurFem': ['vós', 'vós'],\n",
    "    '3PlurMale': ['eles', 'eles'],\n",
    "    '3PlurFem': ['elas', 'elas']\n",
    "}\n",
    "\n",
    "bosque.sentences[\"CP493-7\"].tokens[bosque.sentences[\"CP493-7\"].map_token_id[\"26\"]].deprel = \"advcl\"\n",
    "bosque.sentences[\"CP493-7\"].tokens[bosque.sentences[\"CP493-7\"].map_token_id[\"26\"]].dephead = \"13\"            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_feats(feat, token):\n",
    "    if not feat in token.feats:\n",
    "        return \"\"\n",
    "    return token.feats.split(feat + \"=\")[1].split(\"|\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_phrase(sentence, first_token_id, phrase):\n",
    "    sintagmas_anteriores = []\n",
    "    for t, token in enumerate(sentence.tokens):\n",
    "        if t < sentence.map_token_id[first_token_id]:\n",
    "            if token.dephead == first_token_id and (token.upos == \"ADV\" or (token.lemma == \"se\" and token.upos == \"PRON\") or token.deprel == \"cop\" or token.upos == \"AUX\"):\n",
    "                is_sintagma = True\n",
    "                for _token in sentence.tokens:\n",
    "                    if _token.upos == \"PUNCT\" and _token.dephead == token.id:\n",
    "                        is_sintagma = False\n",
    "                        break\n",
    "                if is_sintagma:\n",
    "                    sintagmas_anteriores.append(sentence.tokens[t].id)\n",
    "    who_head = first_token_id\n",
    "    first_token_id = min(sintagmas_anteriores) if sintagmas_anteriores else first_token_id\n",
    "\n",
    "    for new_token in reversed(phrase):\n",
    "        token = estrutura_ud.Token()\n",
    "        token.build(new_token.to_str())\n",
    "        token.dephead = str(int(token.dephead) - int(token.id) + int(who_head.split(\"-\")[0]))\n",
    "        token.id = first_token_id.split(\"-\")[0]\n",
    "        if first_token_id == \"1\":\n",
    "            if token.word != \"<SUBJ>\":\n",
    "                token.word = token.word.title()\n",
    "            sentence.tokens[0].word = sentence.tokens[0].word.lower()\n",
    "        token.misc = \"|\".join(sorted([x for x in token.misc.split(\"|\") + [\"<SUBJ>\"] if x != \"_\"]))\n",
    "        sentence.tokens.insert(sentence.map_token_id[first_token_id], token)\n",
    "        \n",
    "        t_first_token_id = sentence.map_token_id[first_token_id]\n",
    "        for t, token in enumerate(sentence.tokens):\n",
    "            if t > t_first_token_id:\n",
    "                token.id = str(int(token.id)+1) if not '-' in token.id else str(int(token.id.split(\"-\")[0])+1) + \"-\" + str(int(token.id.split(\"-\")[1])+1)\n",
    "            sentence.map_token_id[token.id] = t\n",
    "        for t, token in enumerate(sentence.tokens):\n",
    "            if token.dephead not in [\"0\", \"_\"] and sentence.map_token_id[token.dephead] >= sentence.map_token_id[first_token_id]:\n",
    "                token.dephead = str(int(token.dephead)+1)\n",
    "        \n",
    "    sentence.refresh_map_token_id()\n",
    "                      \n",
    "    sentence.metadados['text'] = \" \".join([x.word for x in sentence.tokens if not '-' in x.id])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ADVCL à esquerda com mesmo PESSNUM (root)\n",
    "reconstituidos = []\n",
    "for sent_id, tokens in list(root.items()):\n",
    "    sentence = bosque.sentences[sent_id]\n",
    "    sentence_qualifies = False\n",
    "    to_add = {}\n",
    "    for t in tokens:\n",
    "        for _t, _token in enumerate(sentence.tokens):\n",
    "            if _token.deprel == \"advcl\" and sentence.tokens[t].id == _token.dephead and _t < t and find_feats(\"Person\", _token) == find_feats(\"Person\", sentence.tokens[t]) and find_feats(\"Number\", _token) == find_feats(\"Number\", sentence.tokens[t]):\n",
    "                for __token in sentence.tokens:\n",
    "                    if __token.deprel in [\"nsubj\", \"csubj\", \"nsubj:pass\"] and __token.dephead == _token.id:\n",
    "                        sentence_qualifies = True\n",
    "                        to_add[t] = []\n",
    "                        for ___token in sentence.tokens:\n",
    "                            if ___token.dephead == __token.id or ___token.id == __token.id:\n",
    "                                to_add[t].append(___token)\n",
    "                        break\n",
    "    \n",
    "    if sentence_qualifies:\n",
    "        for i, t in enumerate(to_add):\n",
    "            bosque.sentences[sent_id] = insert_phrase(bosque.sentences[sent_id], str(int(bosque.sentences[sent_id].tokens[t+i].id)), to_add[t])\n",
    "        del root[sent_id]\n",
    "        reconstituidos.append(sent_id + \":\\n\" + bosque.sentences[sent_id].text + \"\\n\" + bosque.sentences[sent_id].metadados['text'])\n",
    "\n",
    "print(\"Reconstituídos: {} / {}\".format(len(reconstituidos), len_root))\n",
    "print(\"\\n\" + \"\\n\\n\".join(reconstituidos[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ADVCL à esquerda com mesmo PESSNUM (subordinate)\n",
    "reconstituidos = []\n",
    "for sent_id, tokens in list(subordinate.items()):\n",
    "    sentence = bosque.sentences[sent_id]\n",
    "    sentence_qualifies = False\n",
    "    to_add = {}\n",
    "    for t in tokens:\n",
    "        for _t, _token in enumerate(sentence.tokens):\n",
    "            if _token.deprel == \"advcl\" and sentence.tokens[t].id == _token.dephead and _t < t and find_feats(\"Person\", _token) == find_feats(\"Person\", sentence.tokens[t]) and find_feats(\"Number\", _token) == find_feats(\"Number\", sentence.tokens[t]):\n",
    "                for __token in sentence.tokens:\n",
    "                    if __token.deprel in [\"nsubj\", \"csubj\", \"nsubj:pass\"] and __token.dephead == _token.id:\n",
    "                        sentence_qualifies = True\n",
    "                        to_add[t] = []\n",
    "                        for ___token in sentence.tokens:\n",
    "                            if ___token.dephead == __token.id or ___token.id == __token.id:\n",
    "                                to_add[t].append(___token)\n",
    "                        break\n",
    "    \n",
    "    if sentence_qualifies:\n",
    "        for i, t in enumerate(to_add):\n",
    "            bosque.sentences[sent_id] = insert_phrase(bosque.sentences[sent_id], str(int(bosque.sentences[sent_id].tokens[t+i].id)), to_add[t])\n",
    "        del subordinate[sent_id]\n",
    "        reconstituidos.append(sent_id + \":\\n\" + bosque.sentences[sent_id].text + \"\\n\" + bosque.sentences[sent_id].metadados['text'])\n",
    "\n",
    "print(\"Reconstituídos: {} / {}\".format(len(reconstituidos), len_subordinate))\n",
    "print(\"\\n\" + \"\\n\\n\".join(reconstituidos[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DESINÊNCIA (root)\n",
    "reconstituidos = []\n",
    "for sent_id, tokens in list(root.items()):\n",
    "    sentence = bosque.sentences[sent_id]\n",
    "    to_add = {}\n",
    "    for i, t in enumerate(tokens):\n",
    "        has_aux = False\n",
    "        for _t, _token in enumerate(sentence.tokens):\n",
    "            if _token.dephead == sentence.tokens[t].id and _token.upos == \"AUX\":\n",
    "                person = find_feats(\"Person\", sentence.tokens[_t])\n",
    "                number = find_feats(\"Number\", sentence.tokens[_t])\n",
    "                gender = find_feats(\"Gender\", sentence.tokens[_t])\n",
    "                has_aux = True\n",
    "                break\n",
    "        if not has_aux:\n",
    "            person = find_feats(\"Person\", sentence.tokens[t])\n",
    "            number = find_feats(\"Number\", sentence.tokens[t])\n",
    "            gender = find_feats(\"Gender\", sentence.tokens[t])\n",
    "        feats = []\n",
    "        if person:\n",
    "            feats.append(\"Person=\" + person)\n",
    "        if number:\n",
    "            feats.append(\"Number=\" + number)\n",
    "        if gender:\n",
    "            feats.append(\"Gender=\" + gender)\n",
    "        feats = \"|\".join(sorted(feats)) if feats else \"_\"\n",
    "        pronoun = person + number + gender\n",
    "        if pronoun in dic_pronouns:\n",
    "            pronoun = [\"1\", dic_pronouns[pronoun][0], dic_pronouns[pronoun][1], 'PRON', '_', feats, \"1\", 'nsubj', '_', '_']\n",
    "        else:\n",
    "            pronoun = [\"1\", \"<SUBJ>\", \"<SUBJ>\", \"NOUN\", \"_\", feats, \"1\", \"nsubj\", \"_\", \"_\"]\n",
    "        new_token = estrutura_ud.Token()\n",
    "        new_token.build(\"\\t\".join(pronoun))\n",
    "    \n",
    "        bosque.sentences[sent_id] = insert_phrase(bosque.sentences[sent_id], str(int(bosque.sentences[sent_id].tokens[t+i].id)), [new_token])\n",
    "    del root[sent_id]\n",
    "    reconstituidos.append(sent_id + \":\\n\" + bosque.sentences[sent_id].text + \"\\n\" + bosque.sentences[sent_id].metadados['text'])\n",
    "\n",
    "print(\"Reconstituídos: {} / {}\".format(len(reconstituidos), len_root))\n",
    "print(\"\\n\" + \"\\n\\n\".join(reconstituidos[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DESINÊNCIA (subordinate)\n",
    "reconstituidos = []\n",
    "for sent_id, tokens in list(subordinate.items()):\n",
    "    sentence = bosque.sentences[sent_id]\n",
    "    to_add = {}\n",
    "    for i, t in enumerate(tokens):\n",
    "        has_aux = False\n",
    "        for _t, _token in enumerate(sentence.tokens):\n",
    "            if _token.dephead == sentence.tokens[t].id and _token.upos == \"AUX\":\n",
    "                person = find_feats(\"Person\", sentence.tokens[_t])\n",
    "                number = find_feats(\"Number\", sentence.tokens[_t])\n",
    "                gender = find_feats(\"Gender\", sentence.tokens[_t])\n",
    "                has_aux = True\n",
    "                break\n",
    "        if not has_aux:\n",
    "            person = find_feats(\"Person\", sentence.tokens[t])\n",
    "            number = find_feats(\"Number\", sentence.tokens[t])\n",
    "            gender = find_feats(\"Gender\", sentence.tokens[t])\n",
    "        feats = []\n",
    "        if person:\n",
    "            feats.append(\"Person=\" + person)\n",
    "        if number:\n",
    "            feats.append(\"Number=\" + number)\n",
    "        if gender:\n",
    "            feats.append(\"Gender=\" + gender)\n",
    "        feats = \"|\".join(sorted(feats)) if feats else \"_\"\n",
    "        pronoun = person + number + gender\n",
    "        if pronoun in dic_pronouns:\n",
    "            pronoun = [\"1\", dic_pronouns[pronoun][0], dic_pronouns[pronoun][1], 'PRON', '_', feats, \"1\", 'nsubj', '_', '_']\n",
    "        else:\n",
    "            pronoun = [\"1\", \"<SUBJ>\", \"<SUBJ>\", \"NOUN\", \"_\", feats, \"1\", \"nsubj\", \"_\", \"_\"]\n",
    "        new_token = estrutura_ud.Token()\n",
    "        new_token.build(\"\\t\".join(pronoun)) \n",
    "\n",
    "        bosque.sentences[sent_id] = insert_phrase(bosque.sentences[sent_id], bosque.sentences[sent_id].tokens[t+i].id, [new_token])\n",
    "    del subordinate[sent_id]\n",
    "    reconstituidos.append(sent_id + \":\\n\" + bosque.sentences[sent_id].text + \"\\n\" + bosque.sentences[sent_id].metadados['text'])\n",
    "\n",
    "print(\"Reconstituídos: {} / {}\".format(len(reconstituidos), len_subordinate))\n",
    "print(\"\\n\" + \"\\n\\n\".join(reconstituidos[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar modificado\n",
    "bosque.save(\"bosque-ud-2.6-desocultado.conllu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpora['obras'].sentences[\"540\"].to_str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#root_nominals[\"obras\"]\n",
    "root_without_subject[\"obras\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
